{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Spark in Scala\n",
    "This notebook is designed to introduce some basic concepts and help get you familiar with using Spark in Scala.  \n",
    "\n",
    "In this notebook, we will load and explore the mtcars dataset. Specifically, this tutorial covers:\n",
    "\n",
    "1. Loading data in memory\n",
    "1. Creating SQLContext\n",
    "1. Creating Spark DataFrame\n",
    "1. Group data by columns \n",
    "1. Operating on columns\n",
    "1. Running SQL Queries from a Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in a DataFrame\n",
    "To create a Spark DataFrame we load an external DataFrame, called `mtcars`. This DataFrame includes 32 observations on 11 variables.\n",
    "\n",
    "[, 1]\tmpg\tMiles/(US) --> gallon  \n",
    "[, 2]\tcyl\t--> Number of cylinders  \n",
    "[, 3]\tdisp\t--> Displacement (cu.in.)  \n",
    "[, 4]\thp -->\tGross horsepower  \n",
    "[, 5]\tdrat -->\tRear axle ratio  \n",
    "[, 6]\twt -->\tWeight (lb/1000)  \n",
    "[, 7]\tqsec -->\t1/4 mile time  \n",
    "[, 8]\tvs -->\tV/S  \n",
    "[, 9]\tam -->\tTransmission (0 = automatic, 1 = manual)  \n",
    "[,10]\tgear -->\tNumber of forward gears  \n",
    "[,11]\tcarb -->\tNumber of carburetors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "import java.net.URL\n",
    "import java.io.File\n",
    "\n",
    "def fileDownloader(url: String, filename: String) = {\n",
    "    new URL(url) #> new File(filename) !!\n",
    "}\n",
    "\n",
    "fileDownloader(\"https://ibm.box.com/shared/static/f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv\", \"/resources/mtcars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SQLContext\n",
    "To work with dataframes we need a SQLContext which is created using `SQLContext(sc)`. SQLContext uses SparkContext which has been already created, named `sc`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import au.com.bytecode.opencsv.CSVParser\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark DataFrames\n",
    "With SQLContext and a loaded local DataFrame, we create a Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Define the schema using a case class.\n",
    "case class Cars(car: String, mpg: String, cyl: String, disp: String, hp: String, drat: String, wt: String, qsec: String, vs: String, am: String, gear: String, carb: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val csv = sc.textFile(\"/resources/mtcars.csv\")\n",
    "val headerAndRows = csv.map(line => line.split(\",\").map(_.trim))\n",
    "val header = headerAndRows.first\n",
    "val data = headerAndRows.filter(_(0) != header(0))\n",
    "val mtcars = data.map(p => Cars(p(0), p(1), p(2), p(3), p(4), p(5), p(6), p(7), p(8), p(9), p(10), p(11))).toDF()\n",
    "mtcars.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displays the content of the DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mtcars.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mtcars.select(\"mpg\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "Filter the DataFrame to only retain rows with `mpg` less than 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mtcars.filter(mtcars(\"mpg\") < 18).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on Columns\n",
    "SparkR also provides a number of functions that can directly applied to columns for data processing and aggregation. The example below shows the use of basic arithmetic functions to convert lb to metric ton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mtcars.withColumn(\"wtTon\", mtcars(\"wt\") * 0.45).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping, Aggregation\n",
    "Spark DataFrames support a number of commonly used functions to aggregate data after grouping. For example we can compute the average weight of cars by their cylinders as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "mtcars.groupBy(\"cyl\").agg(avg(\"wt\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// We can also sort the output from the aggregation to get the most common cars\n",
    "\n",
    "mtcars.groupBy(\"cyl\").agg(count(\"wt\")).sort($\"count(wt)\".desc).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queries from Spark DataFrames\n",
    "A Spark DataFrame can also be registered as a temporary table in Spark SQL and registering a DataFrame as a table allows you to run SQL queries over its data. The `sql` function enables applications to run SQL queries programmatically and returns the result as a DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Register this DataFrame as a table.\n",
    "mtcars.registerTempTable(\"cars\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by sqlContext.\n",
    "val highgearcars = sqlContext.sql(\"SELECT gear FROM cars WHERE cyl >= 4 AND cyl <= 9\")\n",
    "highgearcars.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NOTE: This tutorial draws heavily from the original \n",
    "[Spark Quick Start Guide](http://spark.apache.org/docs/latest/quick-start.html)"
   ]
  },
 
